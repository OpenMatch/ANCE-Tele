{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d8505-c062-4443-a3f8-c093c604ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.tensor([2,2,4])\n",
    "\n",
    "torch.pow(aa, exponent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af87bf8e-6f20-4dd0-adcc-aa39434e16fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern match found 10 files; loading them into index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading shards into index: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2,3'\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def convert_id(look_up_list):\n",
    "    converted_look_up = {}\n",
    "    for idx, ids in enumerate(look_up_list):\n",
    "        converted_look_up[ids] = idx\n",
    "    return converted_look_up\n",
    "\n",
    "def load_trec(file_path):\n",
    "    qid2dids = defaultdict(list)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fi:\n",
    "        for line in tqdm(fi):\n",
    "            qid, did, _ = line.strip().split()\n",
    "            qid2dids[qid].append(did)\n",
    "    return qid2dids\n",
    "\n",
    "\n",
    "\n",
    "class build_dataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        qid2dids,\n",
    "        qid2index,\n",
    "        docid2index,\n",
    "        q_vectors,\n",
    "        d_vectors,\n",
    "    ):\n",
    "        self.qid2dids = qid2dids\n",
    "        self.qid2index = qid2index\n",
    "        self.docid2index = docid2index\n",
    "        self.q_vectors = q_vectors\n",
    "        self.d_vectors = d_vectors\n",
    "    \n",
    "        self.qids = list(qid2dids.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        qid = self.qids[index]\n",
    "        dids = self.qid2dids[qid]\n",
    "        return get_item(\n",
    "            qid,\n",
    "            dids,\n",
    "            qid2index=self.qid2index,\n",
    "            docid2index=self.docid2index,\n",
    "            q_vectors=self.q_vectors,\n",
    "            d_vectors=self.d_vectors,\n",
    "        )\n",
    "    \n",
    "    \n",
    "def get_item(\n",
    "    qid,\n",
    "    dids,\n",
    "    qid2index,\n",
    "    docid2index,\n",
    "    q_vectors,\n",
    "    d_vectors,\n",
    "):\n",
    "    q_index = qid2index[qid]\n",
    "    doc_indexs = [docid2index[did] for did in dids]\n",
    "    \n",
    "    q_vecs = q_vectors[q_index]\n",
    "    doc_vecs = [d_vectors[index] for index in doc_indexs]\n",
    "    \n",
    "    return q_vecs, doc_vecs, qid, dids\n",
    "            \n",
    "\n",
    "def batchify_fct(batch):\n",
    "    \n",
    "    q_vecs = [ex[0] for ex in batch]\n",
    "    doc_vecs = [ex[1] for ex in batch]\n",
    "    qids = [ex[2] for ex in batch]\n",
    "    dids = [ex[3] for ex in batch]\n",
    "    \n",
    "    q_tensor = torch.tensor(q_vecs)\n",
    "    doc_tensor = torch.tensor(doc_vecs)\n",
    "\n",
    "    return q_tensor, doc_tensor, qids, dids\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    DATA_DIR = \"/data/private/sunsi/experiments/cocondenser/results/inference.iter-1.self-neg-cocondenser-20k\"\n",
    "    output_path = os.path.join(DATA_DIR, \"dev-mmr.jsonl\")\n",
    "    \n",
    "    trec_path = os.path.join(DATA_DIR, \"dev.rank.tsv\")\n",
    "    query_reps_path = os.path.join(DATA_DIR, \"query/qry.pt\")\n",
    "    passage_reps_path = os.path.join(DATA_DIR, \"corpus/*.pt\")\n",
    "    ## load query2vec\n",
    "    q_reps, q_lookup = pickle_load(query_reps_path)\n",
    "    \n",
    "    ## load passage2vec\n",
    "    index_files = glob.glob(passage_reps_path)\n",
    "    print(f'Pattern match found {len(index_files)} files; loading them into index.')\n",
    "    \n",
    "    ## load retrieval results\n",
    "    qid2dids = load_trec(trec_path)\n",
    "    \n",
    "    p_reps_0, p_lookup_0 = pickle_load(index_files[0])\n",
    "    shards = chain([(p_reps_0, p_lookup_0)], map(pickle_load, index_files[1:]))\n",
    "    if len(index_files) > 1:\n",
    "        shards = tqdm(shards, desc='Loading shards into index', total=len(index_files))    \n",
    "    \n",
    "    p_reps = []\n",
    "    look_up = []\n",
    "    for _p_reps, p_lookup in shards:\n",
    "        p_reps.append(_p_reps)\n",
    "        look_up += p_lookup\n",
    "      \n",
    "    p_reps = np.concatenate(p_reps, axis=0)\n",
    "    \n",
    "    ## id2index\n",
    "    qid2index = convert_id(q_lookup)\n",
    "    docid2index = convert_id(look_up)\n",
    "    \n",
    "    ## dataset\n",
    "    encode_dataset = build_dataset(\n",
    "        qid2dids=qid2dids,\n",
    "        qid2index=qid2index,\n",
    "        docid2index=docid2index,\n",
    "        q_vectors=q_reps,\n",
    "        d_vectors=p_reps,\n",
    "    )\n",
    "    sampler = torch.utils.data.sampler.SequentialSampler(encode_dataset)\n",
    "    \n",
    "    \n",
    "    \n",
    "    batch_size = 128\n",
    "    encode_data_loader = DataLoader(\n",
    "        encode_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=16,\n",
    "        collate_fn=batchify_fct,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as fw:\n",
    "    \n",
    "        batch_iterator = tqdm(encode_data_loader, desc=\"Iteration\")\n",
    "        for batch in tqdm(batch_iterator):\n",
    "            q_tensor, doc_tensor, qids, dids = batch\n",
    "\n",
    "            q_tensor = q_tensor.cuda() # bz * 768\n",
    "            doc_tensor = doc_tensor.cuda() # bz * topk * 768\n",
    "\n",
    "            qd_score = torch.sum(q_tensor.unsqueeze(1) * doc_tensor, dim=-1)\n",
    "            norm_qd_score = F.normalize(qd_score, p=1, dim=-1)\n",
    "\n",
    "            cos_dd_score = torch.cosine_similarity(doc_tensor.unsqueeze(1), doc_tensor.unsqueeze(2), dim=-1)\n",
    "\n",
    "            norm_qd_score = norm_qd_score.cpu().detach().tolist()\n",
    "            cos_dd_score = cos_dd_score.cpu().detach().tolist()\n",
    "\n",
    "            for batch_id, (qid, sub_dids) in enumerate(zip(qids, dids)):\n",
    "                save_item = {\n",
    "                    \"qid\":qid,\n",
    "                    \"dids\":sub_dids,\n",
    "                    \"qd_score\":norm_qd_score,\n",
    "                    \"dd_score\":cos_dd_score,\n",
    "                }\n",
    "                fw.write(json.dumps(save_item)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "74be704c-4a8e-48af-8240-d42692d47beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|                                                                                                                                       | 0/7 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                  | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  14%|██████████████████▏                                                                                                            | 1/7 [01:08<06:53, 68.96s/it]\u001b[A\n",
      "Iteration:  29%|████████████████████████████████████▎                                                                                          | 2/7 [02:13<05:30, 66.11s/it]\u001b[A\n",
      "Iteration:  43%|██████████████████████████████████████████████████████▍                                                                        | 3/7 [03:18<04:22, 65.57s/it]\u001b[A\n",
      "Iteration:  57%|████████████████████████████████████████████████████████████████████████▌                                                      | 4/7 [04:22<03:15, 65.32s/it]\u001b[A\n",
      "Iteration:  71%|██████████████████████████████████████████████████████████████████████████████████████████▋                                    | 5/7 [05:28<02:10, 65.33s/it]\u001b[A\n",
      "Iteration:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                  | 6/7 [06:31<01:04, 64.75s/it]\u001b[A\n",
      "Iteration: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:14<00:00, 62.08s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [07:14<00:00, 62.08s/it]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocondenser",
   "language": "python",
   "name": "cocondenser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
